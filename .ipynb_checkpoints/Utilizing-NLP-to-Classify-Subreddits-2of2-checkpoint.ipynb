{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART TWO \n",
    "\n",
    "## Content List- Part 2\n",
    "\n",
    "- [Data Cleaning and EDA](#Data-Cleaning-and-EDA)\n",
    "- [Preprocessing and Modeling](#Preprocessing-and-Modeling)\n",
    "- [Evaluation and Conceptual Understanding](#Evaluation-and-Conceptual-Understanding)\n",
    "- [Conclusion and Recommendations](#Conclusion-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:48.803327Z",
     "start_time": "2019-07-07T20:19:47.709238Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import text, stop_words\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Dataframe from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:48.825869Z",
     "start_time": "2019-07-07T20:19:48.806812Z"
    }
   },
   "outputs": [],
   "source": [
    "master_df = pd.read_csv('./data/master_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:48.846472Z",
     "start_time": "2019-07-07T20:19:48.828495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1799"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:48.865819Z",
     "start_time": "2019-07-07T20:19:48.852008Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "Length of Title    0\n",
       "Post Text          0\n",
       "Subreddit          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for nulls\n",
    "master_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:48.951354Z",
     "start_time": "2019-07-07T20:19:48.941285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1799, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check shape of new, combined dataframe\n",
    "master_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:49.111764Z",
     "start_time": "2019-07-07T20:19:49.100603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Length of Title', 'Post Text', 'Subreddit'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:49.294460Z",
     "start_time": "2019-07-07T20:19:49.285083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108.51917732073375"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df['Length of Title'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:49.506963Z",
     "start_time": "2019-07-07T20:19:49.498141Z"
    }
   },
   "outputs": [],
   "source": [
    "#set feature and targets\n",
    "X = master_df[['Post Text', 'Length of Title']]\n",
    "y = master_df['Subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:49.716032Z",
     "start_time": "2019-07-07T20:19:49.704545Z"
    }
   },
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify= y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Baseline Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Accuracy is our metric, it is vital to determine a baseline so that we can compare our results. We will do this by performing a quick analysis on the distribution of the classes, in order to see if there is any inherent imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:50.520387Z",
     "start_time": "2019-07-07T20:19:50.325211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.551111\n",
       "0    0.448889\n",
       "Name: Subreddit, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline Accuracy\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline Accuracy of 55.58% is important for the model as it provides a metric on which the model should be judged. 55% is the equivalent of random chance pick by the Majority class, even higher than a coin flip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:50.745332Z",
     "start_time": "2019-07-07T20:19:50.735006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1349, 2)\n",
      "(1349,)\n",
      "(450, 2)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "#show us the shape of our data\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amending stop word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:51.172429Z",
     "start_time": "2019-07-07T20:19:51.161074Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n",
      "15\n",
      "341\n",
      "332\n"
     ]
    }
   ],
   "source": [
    "additional_politics_english_stop = ['www', 'things', 'does', 'x200b', 'amp', 'want', 'watch',\n",
    "                           'just', 'like', 'https', 'com', 'republican', 'republicans',\n",
    "                           'libertarians', 'democrats', 'democrat', 'people', 'libertarian',\n",
    "                           'says', 'say', 'did', 'this', 'conservative', 'conservatives' ]\n",
    "\n",
    "additional_english_stop = ['www', 'things', 'does', 'x200b', 'amp',\n",
    "                           'just', 'like', 'https', 'com', 'watch', 'want',\n",
    "                           'says', 'say', 'did', 'this']\n",
    "\n",
    "new_stop_list = stop_words.ENGLISH_STOP_WORDS.union(additional_english_stop)\n",
    "new_politics_english_stop_list = stop_words.ENGLISH_STOP_WORDS.union(additional_politics_english_stop)\n",
    "print(len(stop_words.ENGLISH_STOP_WORDS))\n",
    "print(len(additional_english_stop))\n",
    "print(len(new_politics_english_stop_list))\n",
    "print(len(new_stop_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:19:51.386134Z",
     "start_time": "2019-07-07T20:19:51.374034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'amp',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'com',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'did',\n",
       "           'do',\n",
       "           'does',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'https',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'just',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'like',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'say',\n",
       "           'says',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'things',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'want',\n",
       "           'was',\n",
       "           'watch',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'www',\n",
       "           'x200b',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_stop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline & GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing gridsearch with vectorizer, add onto X_train the feature desired (length of post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:20:34.230827Z",
     "start_time": "2019-07-07T20:19:52.372688Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   22.7s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   41.5s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'cvec__max_features': [None, 500, 1000], 'cvec__min_df': [2, 3], 'cvec__max_df': [0.3, 0.4], 'cvec__ngram_range': [(1, 2), (1, 3)], 'cvec__stop_words': [None, 'english', frozenset({'when', 'although', 'had', 'eleven', 'put', 'seem', 'every', 'former', 'for', 'go', 'whereas', 'cannot', 'n...erywhere', 'becoming', 'even', 'what', 'everything', 'while', 'its', 'de'})], 'lr__penalty': ['l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_cvec_lr = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params_cvec_lr = {\n",
    "    'cvec__max_features': [None,500,1000],\n",
    "    'cvec__min_df': [2,3],\n",
    "    'cvec__max_df': [.3,.4,],\n",
    "    'cvec__ngram_range': [(1,2),(1,3)],\n",
    "    'cvec__stop_words': [None,'english',new_stop_list],\n",
    "    'lr__penalty': ['l2']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_cvec_lr, param_grid=pipe_params_cvec_lr, cv=5,n_jobs = -1,verbose = 1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:20:34.336957Z",
     "start_time": "2019-07-07T20:20:34.234990Z"
    }
   },
   "outputs": [],
   "source": [
    "cvlr_bestscore = gs.best_score_\n",
    "cvlr_params = gs.best_params_\n",
    "cvlr_train = gs.score(X_train[\"Post Text\"],y_train)\n",
    "cvlr_test= gs.score(X_test[\"Post Text\"],y_test)\n",
    "cvlr = ('CountVec with LogReg', cvlr_bestscore, cvlr_params, cvlr_train, cvlr_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:20:34.435864Z",
     "start_time": "2019-07-07T20:20:34.339522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV Score: 0.7835433654558932\n",
      "Best Parameters: {'cvec__max_df': 0.4, 'cvec__max_features': None, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'when', 'although', 'had', 'eleven', 'put', 'seem', 'every', 'former', 'for', 'go', 'whereas', 'cannot', 'nobody', 'serious', 'in', 'over', 'anyone', 'like', 'anywhere', 'one', 'get', 'inc', 'mine', 'afterwards', 'system', 'yet', 'ie', 'her', 're', 'during', 'so', 'hereby', 'the', 'mostly', 'besides', 'again', 'cant', 'itself', 'moreover', 'through', 'done', 'yourselves', 'amongst', 'hereupon', 'whatever', 'say', 'he', 'such', 'empty', 'still', 'made', 'nothing', 'two', 'ever', 'thereupon', 'now', 'but', 'seeming', 'only', 'ltd', 'either', 'where', 'third', 'i', 'be', 'meanwhile', 'seems', 'just', 'and', 'whenever', 'eight', 'onto', 'except', 'next', 'nor', 'may', 'same', 'as', 'can', 'show', 'of', 'an', 'themselves', 'five', 'being', 'my', 'call', 'did', 'https', 'front', 'con', 'they', 'your', 'is', 'upon', 'none', 'above', 'then', 'became', 'per', 'among', 'who', 'move', 'against', 'that', 'around', 'hers', 'indeed', 'fifty', 'all', 'into', 'those', 'enough', 'a', 'below', 'out', 'whether', 'amoungst', 'nine', 'are', 'therefore', 'sincere', 'fill', 'whole', 'does', 'wherever', 'find', 'sixty', 'un', 'amount', 'down', 'everyone', 'because', 'been', 'up', 'always', 'whither', 'beside', 'though', 'take', 'cry', 'which', 'were', 'anyway', 'top', 'or', 'detail', 'whereafter', 'more', 'me', 'than', 'herself', 'within', 'thus', 'between', 'anything', 'x200b', 'this', 'full', 'fifteen', 'at', 'on', 'thereafter', 'by', 'she', 'found', 'to', 'bill', 'hereafter', 'formerly', 'behind', 'side', 'was', 'until', 'etc', 'neither', 'has', 'myself', 'eg', 'each', 'perhaps', 'elsewhere', 'things', 'latterly', 'his', 'alone', 'towards', 'com', 'further', 'hundred', 'might', 'nevertheless', 'want', 'there', 'give', 'bottom', 'watch', 'well', 'namely', 'yours', 'must', 'many', 'himself', 'please', 'see', 'nowhere', 'along', 'herein', 'other', 'beyond', 'first', 'beforehand', 'with', 'mill', 'wherein', 'rather', 'seemed', 'you', 'thence', 'ours', 'couldnt', 'almost', 'ten', 'their', 'throughout', 'six', 'fire', 'some', 'becomes', 'thereby', 'them', 'once', 'across', 'under', 'would', 'have', 'own', 'somewhere', 'name', 'whose', 'last', 'off', 'thru', 'less', 'these', 'do', 'if', 'without', 'us', 'whoever', 'noone', 'another', 'part', 'describe', 'also', 'no', 'it', 'twenty', 'something', 'why', 'our', 'interest', 'him', 'much', 'back', 'since', 'how', 'whence', 'thick', 'few', 'toward', 'hasnt', 'become', 'three', 'am', 'before', 'twelve', 'after', 'together', 'anyhow', 'hence', 'already', 'others', 'any', 'too', 'due', 'therein', 'yourself', 'we', 'otherwise', 'both', 'www', 'least', 'should', 'somehow', 'keep', 'four', 'here', 'most', 'whom', 'via', 'will', 'could', 'co', 'latter', 'however', 'not', 'amp', 'forty', 'often', 'never', 'sometimes', 'says', 'whereupon', 'very', 'else', 'whereby', 'ourselves', 'someone', 'about', 'from', 'thin', 'sometime', 'several', 'everywhere', 'becoming', 'even', 'what', 'everything', 'while', 'its', 'de'}), 'lr__penalty': 'l2'}\n",
      "Train Accuracy Score: 0.9836916234247591\n",
      "Test Accuracy Score: 0.7955555555555556\n"
     ]
    }
   ],
   "source": [
    "print(f'Best CV Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty strong results with CountVectorizer and Logistic Regression, with a Best CV Score: 0.79581; where the 'cvec__max_df': 0.3, 'cvec__max_features': None, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 3), 'cvec__stop_words':'english', 'lr__penalty': 'l2'. \n",
    "\n",
    "Train Accuracy Score: 0.9798055347793567\n",
    "\n",
    "Test Accuracy Score: 0.8094170403587444\n",
    "\n",
    "The train score of approx 0.9798 was much better than the test score of 0.8094 indicating that this model is overfit despite tuning the hyperparameters and the strong training data score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:20:59.219663Z",
     "start_time": "2019-07-07T20:20:34.441613Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 72 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    8.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7435137138621201\n",
      "Best Parameters: {'lr__penalty': 'l2', 'tvec__max_df': 0.5, 'tvec__max_features': None, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'when', 'although', 'had', 'eleven', 'put', 'seem', 'every', 'former', 'for', 'go', 'whereas', 'cannot', 'nobody', 'serious', 'in', 'over', 'anyone', 'like', 'anywhere', 'one', 'get', 'inc', 'mine', 'afterwards', 'system', 'yet', 'ie', 'her', 're', 'during', 'so', 'hereby', 'the', 'mostly', 'besides', 'again', 'cant', 'itself', 'moreover', 'through', 'done', 'yourselves', 'amongst', 'hereupon', 'whatever', 'say', 'he', 'such', 'empty', 'still', 'made', 'nothing', 'two', 'ever', 'thereupon', 'now', 'but', 'seeming', 'only', 'ltd', 'either', 'where', 'third', 'i', 'be', 'meanwhile', 'seems', 'just', 'and', 'whenever', 'eight', 'onto', 'except', 'next', 'nor', 'may', 'same', 'as', 'can', 'show', 'of', 'an', 'themselves', 'five', 'being', 'my', 'call', 'did', 'https', 'front', 'con', 'they', 'your', 'is', 'upon', 'none', 'above', 'then', 'became', 'per', 'among', 'who', 'move', 'against', 'that', 'around', 'hers', 'indeed', 'fifty', 'all', 'into', 'those', 'enough', 'a', 'below', 'out', 'whether', 'amoungst', 'nine', 'are', 'therefore', 'sincere', 'fill', 'whole', 'does', 'wherever', 'find', 'sixty', 'un', 'amount', 'down', 'everyone', 'because', 'been', 'up', 'always', 'whither', 'beside', 'though', 'take', 'cry', 'which', 'were', 'anyway', 'top', 'or', 'detail', 'whereafter', 'more', 'me', 'than', 'herself', 'within', 'thus', 'between', 'anything', 'x200b', 'this', 'full', 'fifteen', 'at', 'on', 'thereafter', 'by', 'she', 'found', 'to', 'bill', 'hereafter', 'formerly', 'behind', 'side', 'was', 'until', 'etc', 'neither', 'has', 'myself', 'eg', 'each', 'perhaps', 'elsewhere', 'things', 'latterly', 'his', 'alone', 'towards', 'com', 'further', 'hundred', 'might', 'nevertheless', 'want', 'there', 'give', 'bottom', 'watch', 'well', 'namely', 'yours', 'must', 'many', 'himself', 'please', 'see', 'nowhere', 'along', 'herein', 'other', 'beyond', 'first', 'beforehand', 'with', 'mill', 'wherein', 'rather', 'seemed', 'you', 'thence', 'ours', 'couldnt', 'almost', 'ten', 'their', 'throughout', 'six', 'fire', 'some', 'becomes', 'thereby', 'them', 'once', 'across', 'under', 'would', 'have', 'own', 'somewhere', 'name', 'whose', 'last', 'off', 'thru', 'less', 'these', 'do', 'if', 'without', 'us', 'whoever', 'noone', 'another', 'part', 'describe', 'also', 'no', 'it', 'twenty', 'something', 'why', 'our', 'interest', 'him', 'much', 'back', 'since', 'how', 'whence', 'thick', 'few', 'toward', 'hasnt', 'become', 'three', 'am', 'before', 'twelve', 'after', 'together', 'anyhow', 'hence', 'already', 'others', 'any', 'too', 'due', 'therein', 'yourself', 'we', 'otherwise', 'both', 'www', 'least', 'should', 'somehow', 'keep', 'four', 'here', 'most', 'whom', 'via', 'will', 'could', 'co', 'latter', 'however', 'not', 'amp', 'forty', 'often', 'never', 'sometimes', 'says', 'whereupon', 'very', 'else', 'whereby', 'ourselves', 'someone', 'about', 'from', 'thin', 'sometime', 'several', 'everywhere', 'becoming', 'even', 'what', 'everything', 'while', 'its', 'de'})}\n",
      "Train Accuracy Score: 0.9340252038547072\n",
      "Test Accuracy Score: 0.7777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 288 out of 288 | elapsed:   24.6s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pipe_tvec_lr = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params_tvec_lr = {\n",
    "    'tvec__max_features': [None,1000],\n",
    "    'tvec__min_df': [2,3,4],\n",
    "    'tvec__max_df': [.3,.5],\n",
    "    'tvec__ngram_range': [(1,1),(1,3)],\n",
    "    'tvec__stop_words': [None, new_stop_list,'english'],\n",
    "    'lr__penalty': ['l2']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_tvec_lr, param_grid=pipe_params_tvec_lr, cv=4, n_jobs=-1, verbose = 1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n",
    "\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:20:59.313257Z",
     "start_time": "2019-07-07T20:20:59.223948Z"
    }
   },
   "outputs": [],
   "source": [
    "tflr_bestscore = gs.best_score_\n",
    "tflr_params = gs.best_params_\n",
    "tflr_train = gs.score(X_train[\"Post Text\"],y_train)\n",
    "tflr_test= gs.score(X_test[\"Post Text\"],y_test)\n",
    "tflr = ('TF-IDF with LogReg',tflr_bestscore, tflr_params, tflr_train, tflr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:20:59.405542Z",
     "start_time": "2019-07-07T20:20:59.319133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7435137138621201\n",
      "Best Parameters: {'lr__penalty': 'l2', 'tvec__max_df': 0.5, 'tvec__max_features': None, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'when', 'although', 'had', 'eleven', 'put', 'seem', 'every', 'former', 'for', 'go', 'whereas', 'cannot', 'nobody', 'serious', 'in', 'over', 'anyone', 'like', 'anywhere', 'one', 'get', 'inc', 'mine', 'afterwards', 'system', 'yet', 'ie', 'her', 're', 'during', 'so', 'hereby', 'the', 'mostly', 'besides', 'again', 'cant', 'itself', 'moreover', 'through', 'done', 'yourselves', 'amongst', 'hereupon', 'whatever', 'say', 'he', 'such', 'empty', 'still', 'made', 'nothing', 'two', 'ever', 'thereupon', 'now', 'but', 'seeming', 'only', 'ltd', 'either', 'where', 'third', 'i', 'be', 'meanwhile', 'seems', 'just', 'and', 'whenever', 'eight', 'onto', 'except', 'next', 'nor', 'may', 'same', 'as', 'can', 'show', 'of', 'an', 'themselves', 'five', 'being', 'my', 'call', 'did', 'https', 'front', 'con', 'they', 'your', 'is', 'upon', 'none', 'above', 'then', 'became', 'per', 'among', 'who', 'move', 'against', 'that', 'around', 'hers', 'indeed', 'fifty', 'all', 'into', 'those', 'enough', 'a', 'below', 'out', 'whether', 'amoungst', 'nine', 'are', 'therefore', 'sincere', 'fill', 'whole', 'does', 'wherever', 'find', 'sixty', 'un', 'amount', 'down', 'everyone', 'because', 'been', 'up', 'always', 'whither', 'beside', 'though', 'take', 'cry', 'which', 'were', 'anyway', 'top', 'or', 'detail', 'whereafter', 'more', 'me', 'than', 'herself', 'within', 'thus', 'between', 'anything', 'x200b', 'this', 'full', 'fifteen', 'at', 'on', 'thereafter', 'by', 'she', 'found', 'to', 'bill', 'hereafter', 'formerly', 'behind', 'side', 'was', 'until', 'etc', 'neither', 'has', 'myself', 'eg', 'each', 'perhaps', 'elsewhere', 'things', 'latterly', 'his', 'alone', 'towards', 'com', 'further', 'hundred', 'might', 'nevertheless', 'want', 'there', 'give', 'bottom', 'watch', 'well', 'namely', 'yours', 'must', 'many', 'himself', 'please', 'see', 'nowhere', 'along', 'herein', 'other', 'beyond', 'first', 'beforehand', 'with', 'mill', 'wherein', 'rather', 'seemed', 'you', 'thence', 'ours', 'couldnt', 'almost', 'ten', 'their', 'throughout', 'six', 'fire', 'some', 'becomes', 'thereby', 'them', 'once', 'across', 'under', 'would', 'have', 'own', 'somewhere', 'name', 'whose', 'last', 'off', 'thru', 'less', 'these', 'do', 'if', 'without', 'us', 'whoever', 'noone', 'another', 'part', 'describe', 'also', 'no', 'it', 'twenty', 'something', 'why', 'our', 'interest', 'him', 'much', 'back', 'since', 'how', 'whence', 'thick', 'few', 'toward', 'hasnt', 'become', 'three', 'am', 'before', 'twelve', 'after', 'together', 'anyhow', 'hence', 'already', 'others', 'any', 'too', 'due', 'therein', 'yourself', 'we', 'otherwise', 'both', 'www', 'least', 'should', 'somehow', 'keep', 'four', 'here', 'most', 'whom', 'via', 'will', 'could', 'co', 'latter', 'however', 'not', 'amp', 'forty', 'often', 'never', 'sometimes', 'says', 'whereupon', 'very', 'else', 'whereby', 'ourselves', 'someone', 'about', 'from', 'thin', 'sometime', 'several', 'everywhere', 'becoming', 'even', 'what', 'everything', 'while', 'its', 'de'})}\n",
      "Train Accuracy Score: 0.9340252038547072\n",
      "Test Accuracy Score: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for TFIDF and Logistic Regression, with a Best cv score of ~0.7644; where the optimal parameters were 'tvec__max_df': 0.3, 'tvec__max_features': None, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 3), 'tvec__stop_words': new_stop_list, 'lr__penalty': 'l2'.\n",
    "\n",
    "Train Accuracy Score: 0.9281974569932685\n",
    "\n",
    "Test Accuracy Score: 0.7982062780269058\n",
    "\n",
    "The train score was better than the test score indicating that this model is overfit despite tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:24:24.517979Z",
     "start_time": "2019-07-07T20:23:27.377255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 144 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=4)]: Done 576 out of 576 | elapsed:   56.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('mnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=4,\n",
       "       param_grid={'cvec__max_features': [None, 500, 1000, 2500], 'cvec__min_df': [2, 3], 'cvec__max_df': [0.4, 0.8], 'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)], 'cvec__stop_words': [None, frozenset({'when', 'although', 'had', 'eleven', 'put', 'seem', 'every', 'former', 'for', 'go', 'whereas', 'cannot',...everal', 'everywhere', 'becoming', 'even', 'what', 'everything', 'while', 'its', 'de'}), 'english']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_cvec_mnb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_params_cvec_mnb = {\n",
    "    'cvec__max_features': [None,500,1000,2500],\n",
    "    'cvec__min_df': [2,3],\n",
    "    'cvec__max_df': [.4, .8],\n",
    "    'cvec__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "    'cvec__stop_words': [None, new_stop_list,'english']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_cvec_mnb, param_grid=pipe_params_cvec_mnb, cv=4, n_jobs = 4, verbose = 1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:24:24.700425Z",
     "start_time": "2019-07-07T20:24:24.526997Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvmnb_bestscore = gs.best_score_\n",
    "cvmnb_params = gs.best_params_\n",
    "cvmnb_train = gs.score(X_train[\"Post Text\"],y_train)\n",
    "cvmnb_test= gs.score(X_test[\"Post Text\"],y_test)\n",
    "cvmnb = ('CountVec with MNB',cvmnb_bestscore, cvmnb_params, cvmnb_train, cvmnb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:22:31.580848Z",
     "start_time": "2019-07-07T20:22:31.480124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7086730911786508\n",
      "Best Parameters: {'cvec__max_df': 0.4, 'cvec__max_features': None, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 3), 'cvec__stop_words': 'english'}\n",
      "Train Accuracy Score: 0.9154929577464789\n",
      "Test Accuracy Score: 0.7488888888888889\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer and Multinomial Naive Bayes, with a Best cv score of 0.7255; where the optimal parameters were 'cvec__max_df': 0.4, 'cvec__max_features': None, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 3), 'cvec__stop_words': 'english'.\n",
    "\n",
    "Train Accuracy Score: 0.8833208676140614\n",
    "\n",
    "Test Accuracy Score: 0.7556053811659192\n",
    "\n",
    "The train score of approx 0.8833 was much better than the test score of 0.7556 indicating that this model is very overfit despite tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:29:21.776875Z",
     "start_time": "2019-07-07T20:27:55.488734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 216 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   43.0s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...rue,\n",
       "        vocabulary=None)), ('mnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'tvec__max_features': [None, 500, 1000, 3000], 'tvec__min_df': [2, 3], 'tvec__max_df': [0.2, 0.3, 0.4], 'tvec__ngram_range': [(1, 1), (1, 2), (1, 3)], 'tvec__stop_words': [None, frozenset({'when', 'although', 'had', 'eleven', 'put', 'seem', 'every', 'former', 'for', 'go', 'whereas', 'can...everal', 'everywhere', 'becoming', 'even', 'what', 'everything', 'while', 'its', 'de'}), 'english']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_tvec_mnb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_params_tvec_mnb = {\n",
    "    'tvec__max_features': [None,500,1000,3000],\n",
    "    'tvec__min_df': [2,3],\n",
    "    'tvec__max_df': [.2,.3,.4,],\n",
    "    'tvec__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "    'tvec__stop_words': [None, new_stop_list,'english']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_tvec_mnb, param_grid=pipe_params_tvec_mnb, cv=4, n_jobs = -1, verbose = 1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:29:21.910964Z",
     "start_time": "2019-07-07T20:29:21.784211Z"
    }
   },
   "outputs": [],
   "source": [
    "tfmnb_bestscore = gs.best_score_\n",
    "tfmnb_params = gs.best_params_\n",
    "tfmnb_train = gs.score(X_train[\"Post Text\"],y_train)\n",
    "tfmnb_test= gs.score(X_test[\"Post Text\"],y_test)\n",
    "\n",
    "tfmnb = ('TF-IDF with MNB',tfmnb_bestscore, tfmnb_params, tfmnb_train, tfmnb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:29:22.027031Z",
     "start_time": "2019-07-07T20:29:21.913179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7131208302446257\n",
      "Best Parameters: {'tvec__max_df': 0.4, 'tvec__max_features': None, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'when', 'although', 'had', 'eleven', 'put', 'seem', 'every', 'former', 'for', 'go', 'whereas', 'cannot', 'nobody', 'serious', 'in', 'over', 'anyone', 'like', 'anywhere', 'one', 'get', 'inc', 'mine', 'afterwards', 'system', 'yet', 'ie', 'her', 're', 'during', 'so', 'hereby', 'the', 'mostly', 'besides', 'again', 'cant', 'itself', 'moreover', 'through', 'done', 'yourselves', 'amongst', 'hereupon', 'whatever', 'say', 'he', 'such', 'empty', 'still', 'made', 'nothing', 'two', 'ever', 'thereupon', 'now', 'but', 'seeming', 'only', 'ltd', 'either', 'where', 'third', 'i', 'be', 'meanwhile', 'seems', 'just', 'and', 'whenever', 'eight', 'onto', 'except', 'next', 'nor', 'may', 'same', 'as', 'can', 'show', 'of', 'an', 'themselves', 'five', 'being', 'my', 'call', 'did', 'https', 'front', 'con', 'they', 'your', 'is', 'upon', 'none', 'above', 'then', 'became', 'per', 'among', 'who', 'move', 'against', 'that', 'around', 'hers', 'indeed', 'fifty', 'all', 'into', 'those', 'enough', 'a', 'below', 'out', 'whether', 'amoungst', 'nine', 'are', 'therefore', 'sincere', 'fill', 'whole', 'does', 'wherever', 'find', 'sixty', 'un', 'amount', 'down', 'everyone', 'because', 'been', 'up', 'always', 'whither', 'beside', 'though', 'take', 'cry', 'which', 'were', 'anyway', 'top', 'or', 'detail', 'whereafter', 'more', 'me', 'than', 'herself', 'within', 'thus', 'between', 'anything', 'x200b', 'this', 'full', 'fifteen', 'at', 'on', 'thereafter', 'by', 'she', 'found', 'to', 'bill', 'hereafter', 'formerly', 'behind', 'side', 'was', 'until', 'etc', 'neither', 'has', 'myself', 'eg', 'each', 'perhaps', 'elsewhere', 'things', 'latterly', 'his', 'alone', 'towards', 'com', 'further', 'hundred', 'might', 'nevertheless', 'want', 'there', 'give', 'bottom', 'watch', 'well', 'namely', 'yours', 'must', 'many', 'himself', 'please', 'see', 'nowhere', 'along', 'herein', 'other', 'beyond', 'first', 'beforehand', 'with', 'mill', 'wherein', 'rather', 'seemed', 'you', 'thence', 'ours', 'couldnt', 'almost', 'ten', 'their', 'throughout', 'six', 'fire', 'some', 'becomes', 'thereby', 'them', 'once', 'across', 'under', 'would', 'have', 'own', 'somewhere', 'name', 'whose', 'last', 'off', 'thru', 'less', 'these', 'do', 'if', 'without', 'us', 'whoever', 'noone', 'another', 'part', 'describe', 'also', 'no', 'it', 'twenty', 'something', 'why', 'our', 'interest', 'him', 'much', 'back', 'since', 'how', 'whence', 'thick', 'few', 'toward', 'hasnt', 'become', 'three', 'am', 'before', 'twelve', 'after', 'together', 'anyhow', 'hence', 'already', 'others', 'any', 'too', 'due', 'therein', 'yourself', 'we', 'otherwise', 'both', 'www', 'least', 'should', 'somehow', 'keep', 'four', 'here', 'most', 'whom', 'via', 'will', 'could', 'co', 'latter', 'however', 'not', 'amp', 'forty', 'often', 'never', 'sometimes', 'says', 'whereupon', 'very', 'else', 'whereby', 'ourselves', 'someone', 'about', 'from', 'thin', 'sometime', 'several', 'everywhere', 'becoming', 'even', 'what', 'everything', 'while', 'its', 'de'})}\n",
      "Train Accuracy Score: 0.9355077835433655\n",
      "Test Accuracy Score: 0.7466666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Results for TFIDF and Multinomial Naive Bayes, with a Best cv score of 0.7128; where the optimal parameters were 'tvec__max_df': 0.4, 'tvec__max_features': 1000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'.\n",
    "\n",
    "Train Accuracy Score: 0.9012715033657442\n",
    "\n",
    "Test Accuracy Score: 0.7623318385650224\n",
    "\n",
    "The train score of approx 0.9013 was much better than the test score of 0.7623 indicating that this model is very overfit despite tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:35:55.729143Z",
     "start_time": "2019-07-07T20:35:55.688627Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_pipe = Pipeline([\n",
    "        ('cvec', CountVectorizer()),\n",
    "        ('rfc', RandomForestClassifier())])\n",
    "\n",
    "rf_params = [{\n",
    "    'cvec__max_features': [None, 500,1000],\n",
    "    'cvec__min_df': [2,3],\n",
    "    'cvec__max_df': [.3,.4,.8],\n",
    "    'cvec__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "    'rfc__bootstrap': [True],\n",
    "    'rfc__max_features': [.5, .6],\n",
    "    'rfc__min_samples_leaf': [3,6],\n",
    "    'rfc__min_samples_split':[3,6],\n",
    "    'rfc__n_estimators':[10,100]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:52:58.980496Z",
     "start_time": "2019-07-07T20:35:56.202236Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 864 candidates, totalling 3456 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3456 out of 3456 | elapsed: 17.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'cvec__max_features': [None, 500, 1000], 'cvec__min_df': [2, 3], 'cvec__max_df': [0.3, 0.4, 0.8], 'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)], 'rfc__bootstrap': [True], 'rfc__max_features': [0.5, 0.6], 'rfc__min_samples_leaf': [3, 6], 'rfc__min_samples_split': [3, 6], 'rfc__n_estimators': [10, 100]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(rf_pipe, \n",
    "                   param_grid=rf_params, \n",
    "                   cv = 4,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:52:59.104187Z",
     "start_time": "2019-07-07T20:52:58.987322Z"
    }
   },
   "outputs": [],
   "source": [
    "cvrf_bestscore = gs.best_score_\n",
    "cvrf_params = gs.best_params_\n",
    "cvrf_train = gs.score(X_train[\"Post Text\"],y_train)\n",
    "cvrf_test= gs.score(X_test[\"Post Text\"],y_test)\n",
    "\n",
    "cvrf = ('CountVec with RandomForest',cvrf_bestscore, cvrf_params, cvrf_train, cvrf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:52:59.197869Z",
     "start_time": "2019-07-07T20:52:59.105901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7568569310600445\n",
      "Best Parameters: {'cvec__max_df': 0.4, 'cvec__max_features': 1000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'rfc__bootstrap': True, 'rfc__max_features': 0.5, 'rfc__min_samples_leaf': 3, 'rfc__min_samples_split': 3, 'rfc__n_estimators': 10}\n",
      "Train Accuracy Score: 0.8799110452186805\n",
      "Test Accuracy Score: 0.7755555555555556\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model drastically improved on variance with the combination of CountVectorizer and RandomForestClassifier. The ideal param: were as follows: 'cvec__max_df': 0.9, 'cvec__max_features': None, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'rfc__bootstrap': True, 'rfc__max_features': 0.5, 'rfc__min_samples_leaf': 4, 'rfc__min_samples_split': 3, 'rfc__n_estimators': 100}\n",
    "\n",
    "Train Accuracy Score: 0.868362004487659\n",
    "\n",
    "Test Accuracy Score: 0.757847533632287\n",
    "\n",
    "Furthermore, the fact that the train accuracy score is still higher than the test accuracy score indicates the model is still overfit, albeit suffering from a lower bias as well as a lower variance than the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:06:14.870180Z",
     "start_time": "2019-07-07T21:06:14.862858Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([\n",
    "        ('tvec', TfidfVectorizer()),\n",
    "        ('rfc', RandomForestClassifier())])\n",
    "\n",
    "rf_params = [{\n",
    "    'tvec__max_features': [None],\n",
    "    'tvec__min_df': [2,4],\n",
    "    'tvec__max_df': [.3,.4, .5],\n",
    "    'tvec__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "    'tvec__stop_words': [None],\n",
    "    'rfc__bootstrap': [False, True],\n",
    "    'rfc__n_estimators': [10,100],\n",
    "    'rfc__max_features': [.5, .6, .7],\n",
    "    'rfc__min_samples_leaf': [10],\n",
    "    'rfc__min_samples_split':[3]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:11:53.335846Z",
     "start_time": "2019-07-07T21:06:15.061788Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 216 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=3)]: Done 864 out of 864 | elapsed:  5.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=3,\n",
       "       param_grid=[{'tvec__max_features': [None], 'tvec__min_df': [2, 4], 'tvec__max_df': [0.3, 0.4, 0.5], 'tvec__ngram_range': [(1, 1), (1, 2), (1, 3)], 'tvec__stop_words': [None], 'rfc__bootstrap': [False, True], 'rfc__n_estimators': [10, 100], 'rfc__max_features': [0.5, 0.6, 0.7], 'rfc__min_samples_leaf': [10], 'rfc__min_samples_split': [3]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs= GridSearchCV(rf_pipe, \n",
    "                   param_grid=rf_params, \n",
    "                   cv = 4,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = 3)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:11:53.517466Z",
     "start_time": "2019-07-07T21:11:53.340363Z"
    }
   },
   "outputs": [],
   "source": [
    "tfrf_bestscore = gs.best_score_\n",
    "tfrf_params = gs.best_params_\n",
    "tfrf_train = gs.score(X_train[\"Post Text\"],y_train)\n",
    "tfrf_test= gs.score(X_test[\"Post Text\"],y_test)\n",
    "tfrf = ('TF-IDF with RandomForest', tfrf_bestscore, tfrf_params, tfrf_train, tfrf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:11:53.669911Z",
     "start_time": "2019-07-07T21:11:53.519311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7064492216456635\n",
      "Best Parameters: {'rfc__bootstrap': False, 'rfc__max_features': 0.5, 'rfc__min_samples_leaf': 10, 'rfc__min_samples_split': 3, 'rfc__n_estimators': 100, 'tvec__max_df': 0.3, 'tvec__max_features': None, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': None}\n",
      "Train Accuracy Score: 0.8346923647146034\n",
      "Test Accuracy Score: 0.7355555555555555\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This models score with the combination of TFIDF and RandomForestClassifier average of .7457 was a little lower than the prior model.\n",
    "\n",
    "The ideal paramaters were as follows:{'rfc__bootstrap': False, 'rfc__max_features': 0.5, 'rfc__min_samples_leaf': 10, 'rfc__min_samples_split': 3, 'rfc__n_estimators': 100, 'tvec__max_df': 0.3, 'tvec__max_features': None, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': None.\n",
    "\n",
    "Train Accuracy Score: 0.8160059835452506\n",
    "\n",
    "Test Accuracy Score: 0.7309417040358744\n",
    "\n",
    "Furthermore, the fact that the train accuracy score is still higher than the test accuracy score indicates the model is still overfit (0.78608 vs 0.7511) , albeit suffering from a lower bias as well as a lower variance than the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:11:53.679789Z",
     "start_time": "2019-07-07T21:11:53.674093Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:16:29.475242Z",
     "start_time": "2019-07-07T21:11:53.689304Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...m='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'cvec__max_features': [None, 500, 1000], 'cvec__min_df': [3, 5], 'cvec__max_df': [0.4, 0.3], 'cvec__ngram_range': [(1, 2), (2, 3), (1, 3)], 'cvec__stop_words': [None, 'english', frozenset({'when', 'although', 'had', 'eleven', 'put', 'seem', 'every', 'former', 'for', 'go', 'whereas', 'can...ming', 'even', 'what', 'everything', 'while', 'its', 'de'})], 'ada__learning_rate': [0.3, 0.5, 0.7]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_pipe = Pipeline([\n",
    "        ('cvec', CountVectorizer()),\n",
    "        ('ada', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "ada_params = {\n",
    "    'cvec__max_features': [None,500,1000],\n",
    "    'cvec__min_df': [3,5],\n",
    "    'cvec__max_df': [.4,.3],\n",
    "    'cvec__ngram_range': [(1,2),(2,3),(1,3)],\n",
    "    'cvec__stop_words': [None, 'english', new_stop_list],\n",
    "    'ada__learning_rate': [0.3,.5,.7]}\n",
    "\n",
    "gs= GridSearchCV(ada_pipe, \n",
    "                   param_grid=ada_params, \n",
    "                   cv = 5,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:16:29.636003Z",
     "start_time": "2019-07-07T21:16:29.480852Z"
    }
   },
   "outputs": [],
   "source": [
    "cvada_bestscore = gs.best_score_\n",
    "cvada_params = gs.best_params_\n",
    "cvada_train = gs.score(X_train[\"Post Text\"],y_train)\n",
    "cvada_test= gs.score(X_test[\"Post Text\"],y_test)\n",
    "cvada = ('CountVec with AdaBoost',cvada_bestscore, cvada_params, cvada_train, cvada_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:16:29.796332Z",
     "start_time": "2019-07-07T21:16:29.638202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7368421052631579\n",
      "Best Parameters: {'ada__learning_rate': 0.7, 'cvec__max_df': 0.4, 'cvec__max_features': None, 'cvec__min_df': 3, 'cvec__ngram_range': (1, 3), 'cvec__stop_words': None}\n",
      "Train Accuracy Score: 0.8057820607857672\n",
      "Test Accuracy Score: 0.7711111111111111\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:17:46.270925Z",
     "start_time": "2019-07-07T21:16:29.799176Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 486 out of 486 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...m='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'tvec__max_features': [None, 500, 1000], 'tvec__min_df': [2, 3, 4], 'tvec__max_df': [0.5, 0.4, 0.3], 'tvec__ngram_range': [(1, 1), (1, 3)], 'tvec__stop_words': [None, 'english', frozenset({'when', 'although', 'had', 'eleven', 'put', 'seem', 'every', 'former', 'for', 'go', 'whereas', 'can...re', 'becoming', 'even', 'what', 'everything', 'while', 'its', 'de'})], 'ada__learning_rate': [0.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_pipe = Pipeline([\n",
    "        ('tvec', TfidfVectorizer()),\n",
    "        ('ada', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "ada_params = {\n",
    "    'tvec__max_features': [None,500,1000],\n",
    "    'tvec__min_df': [2,3,4],\n",
    "    'tvec__max_df': [.5,.4,.3],\n",
    "    'tvec__ngram_range': [(1,1),(1,3)],\n",
    "    'tvec__stop_words': [None, 'english', new_stop_list],\n",
    "    'ada__learning_rate': [.5]}\n",
    "\n",
    "gs= GridSearchCV(ada_pipe, \n",
    "                   param_grid=ada_params, \n",
    "                   cv = 3,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:17:46.449003Z",
     "start_time": "2019-07-07T21:17:46.273474Z"
    }
   },
   "outputs": [],
   "source": [
    "tfada_bestscore = gs.best_score_\n",
    "tfada_params = gs.best_params_\n",
    "tfada_train = gs.score(X_train[\"Post Text\"],y_train)\n",
    "tfada_test= gs.score(X_test[\"Post Text\"],y_test)\n",
    "tfada = ('TF-IDF with AdaBoost',tfada_bestscore, tfada_params, tfada_train, tfada_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:17:46.614450Z",
     "start_time": "2019-07-07T21:17:46.450717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7160859896219421\n",
      "Best Parameters: {'ada__learning_rate': 0.5, 'tvec__max_df': 0.5, 'tvec__max_features': 1000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 3), 'tvec__stop_words': None}\n",
      "Train Accuracy Score: 0.7894736842105263\n",
      "Test Accuracy Score: 0.76\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost with TFIDF proved the best so far, with lower variance and higher accuracy with optimal settings of:'ada__learning_rate': 0.5, 'tvec__max_df': 0.5, 'tvec__max_features': 500, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 3), 'tvec__stop_words': new_stop_list.\n",
    "\n",
    "Train Accuracy Score: 0.8032909498878086\n",
    "\n",
    "Test Accuracy Score: 0.7645739910313901\n",
    "\n",
    "Scores show that there is still a tiny bit of overfit, but all in all this model should generalize the best to new data and so we will make our predictions using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:20:29.366323Z",
     "start_time": "2019-07-07T21:17:46.616874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7412898443291327\n",
      "Best Parameters: {'cvec__max_df': 0.4, 'cvec__max_features': 1000, 'cvec__min_df': 3, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': None}\n",
      "Train Accuracy Score: 0.8206078576723499\n",
      "Test Accuracy Score: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "xgb_pipe = Pipeline([\n",
    "        ('cvec', CountVectorizer()),\n",
    "        ('xgb', XGBClassifier())\n",
    "])\n",
    "\n",
    "xgb_params = {\n",
    "    'cvec__max_features': [None,500,1000],\n",
    "    'cvec__min_df': [3,5],\n",
    "    'cvec__max_df': [.4,.3],\n",
    "    'cvec__ngram_range': [(1,2),(2,3),(1,3)],\n",
    "    'cvec__stop_words': [None, 'english', new_stop_list]}\n",
    "\n",
    "gs= GridSearchCV(xgb_pipe, \n",
    "                   param_grid= xgb_params, \n",
    "                   cv = 5,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n",
    "\n",
    "cvxgb_bestscore = gs.best_score_\n",
    "cvxgb_params = gs.best_params_\n",
    "cvxgb_train = gs.score(X_train[\"Post Text\"],y_train)\n",
    "cvxgb_test= gs.score(X_test[\"Post Text\"],y_test)\n",
    "cvxgb = ('CountVec with XGBoost',cvxgb_bestscore, cvxgb_params, cvxgb_train, cvxgb_test)\n",
    "\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:23:19.507597Z",
     "start_time": "2019-07-07T21:20:29.369673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 486 out of 486 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'tvec__max_features': [None, 500, 1000], 'tvec__min_df': [2, 3, 4], 'tvec__max_df': [0.5, 0.4, 0.3], 'tvec__ngram_range': [(1, 1), (1, 3)], 'tvec__stop_words': [None, 'english', frozenset({'when', 'although', 'had', 'eleven', 'put', 'seem', 'every', 'former', 'for', 'go', 'whereas', 'can...metime', 'several', 'everywhere', 'becoming', 'even', 'what', 'everything', 'while', 'its', 'de'})]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_pipe = Pipeline([\n",
    "        ('tvec', TfidfVectorizer()),\n",
    "        ('xgb', XGBClassifier())\n",
    "])\n",
    "\n",
    "xgb_params = {\n",
    "    'tvec__max_features': [None,500,1000],\n",
    "    'tvec__min_df': [2,3,4],\n",
    "    'tvec__max_df': [.5,.4,.3],\n",
    "    'tvec__ngram_range': [(1,1),(1,3)],\n",
    "    'tvec__stop_words': [None, 'english', new_stop_list]}\n",
    "\n",
    "gs= GridSearchCV(xgb_pipe, \n",
    "                   param_grid=xgb_params, \n",
    "                   cv = 3,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:46:01.198114Z",
     "start_time": "2019-07-07T21:46:00.886059Z"
    }
   },
   "outputs": [],
   "source": [
    "tfxgb_bestscore = gs.best_score_\n",
    "tfxgb_params = gs.best_params_\n",
    "tfxgb_train = gs.score(X_train[\"Post Text\"],y_train)\n",
    "tfxgb_test= gs.score(X_test[\"Post Text\"],y_test)\n",
    "tfxgb = ('TF-IDF with XGBoost',tfxgb_bestscore, tfxgb_params, tfxgb_train, tfxgb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:46:08.349469Z",
     "start_time": "2019-07-07T21:46:08.185487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7197924388435878\n",
      "Best Parameters: {'tvec__max_df': 0.5, 'tvec__max_features': 1000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 3), 'tvec__stop_words': frozenset({'when', 'although', 'had', 'eleven', 'put', 'seem', 'every', 'former', 'for', 'go', 'whereas', 'cannot', 'nobody', 'serious', 'in', 'over', 'anyone', 'like', 'anywhere', 'one', 'get', 'inc', 'mine', 'afterwards', 'system', 'yet', 'ie', 'her', 're', 'during', 'so', 'hereby', 'the', 'mostly', 'besides', 'again', 'cant', 'itself', 'moreover', 'through', 'done', 'yourselves', 'amongst', 'hereupon', 'whatever', 'say', 'he', 'such', 'empty', 'still', 'made', 'nothing', 'two', 'ever', 'thereupon', 'now', 'but', 'seeming', 'only', 'ltd', 'either', 'where', 'third', 'i', 'be', 'meanwhile', 'seems', 'just', 'and', 'whenever', 'eight', 'onto', 'except', 'next', 'nor', 'may', 'same', 'as', 'can', 'show', 'of', 'an', 'themselves', 'five', 'being', 'my', 'call', 'did', 'https', 'front', 'con', 'they', 'your', 'is', 'upon', 'none', 'above', 'then', 'became', 'per', 'among', 'who', 'move', 'against', 'that', 'around', 'hers', 'indeed', 'fifty', 'all', 'into', 'those', 'enough', 'a', 'below', 'out', 'whether', 'amoungst', 'nine', 'are', 'therefore', 'sincere', 'fill', 'whole', 'does', 'wherever', 'find', 'sixty', 'un', 'amount', 'down', 'everyone', 'because', 'been', 'up', 'always', 'whither', 'beside', 'though', 'take', 'cry', 'which', 'were', 'anyway', 'top', 'or', 'detail', 'whereafter', 'more', 'me', 'than', 'herself', 'within', 'thus', 'between', 'anything', 'x200b', 'this', 'full', 'fifteen', 'at', 'on', 'thereafter', 'by', 'she', 'found', 'to', 'bill', 'hereafter', 'formerly', 'behind', 'side', 'was', 'until', 'etc', 'neither', 'has', 'myself', 'eg', 'each', 'perhaps', 'elsewhere', 'things', 'latterly', 'his', 'alone', 'towards', 'com', 'further', 'hundred', 'might', 'nevertheless', 'want', 'there', 'give', 'bottom', 'watch', 'well', 'namely', 'yours', 'must', 'many', 'himself', 'please', 'see', 'nowhere', 'along', 'herein', 'other', 'beyond', 'first', 'beforehand', 'with', 'mill', 'wherein', 'rather', 'seemed', 'you', 'thence', 'ours', 'couldnt', 'almost', 'ten', 'their', 'throughout', 'six', 'fire', 'some', 'becomes', 'thereby', 'them', 'once', 'across', 'under', 'would', 'have', 'own', 'somewhere', 'name', 'whose', 'last', 'off', 'thru', 'less', 'these', 'do', 'if', 'without', 'us', 'whoever', 'noone', 'another', 'part', 'describe', 'also', 'no', 'it', 'twenty', 'something', 'why', 'our', 'interest', 'him', 'much', 'back', 'since', 'how', 'whence', 'thick', 'few', 'toward', 'hasnt', 'become', 'three', 'am', 'before', 'twelve', 'after', 'together', 'anyhow', 'hence', 'already', 'others', 'any', 'too', 'due', 'therein', 'yourself', 'we', 'otherwise', 'both', 'www', 'least', 'should', 'somehow', 'keep', 'four', 'here', 'most', 'whom', 'via', 'will', 'could', 'co', 'latter', 'however', 'not', 'amp', 'forty', 'often', 'never', 'sometimes', 'says', 'whereupon', 'very', 'else', 'whereby', 'ourselves', 'someone', 'about', 'from', 'thin', 'sometime', 'several', 'everywhere', 'becoming', 'even', 'what', 'everything', 'while', 'its', 'de'})}\n",
      "Train Accuracy Score: 0.7961452928094885\n",
      "Test Accuracy Score: 0.7444444444444445\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions Utilizing the Top Performing Models\n",
    "#### I would classify two of the models as the best, the one with the highest overall score (lowest bias) and the one with the smallest overall difference between the train and test data (lowest variance). These models are tested out below with their optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:25.196468Z",
     "start_time": "2019-07-07T21:56:25.169964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Length of Title</th>\n",
       "      <th>Post Text</th>\n",
       "      <th>Subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_bu8owc</td>\n",
       "      <td>63</td>\n",
       "      <td>Research Confirms It: Weak Men Are More Likely...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_c926ee</td>\n",
       "      <td>60</td>\n",
       "      <td>The 3 Big Differences Between Conservatives an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_c9upog</td>\n",
       "      <td>273</td>\n",
       "      <td>It's time for trump to do more than lip Servic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_c9lci0</td>\n",
       "      <td>65</td>\n",
       "      <td>Koch Groups Renew Push to Reform Broken U.S....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_c9i6zr</td>\n",
       "      <td>95</td>\n",
       "      <td>30 people have been shot in Chicago since the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  Length of Title  \\\n",
       "0  t3_bu8owc               63   \n",
       "1  t3_c926ee               60   \n",
       "2  t3_c9upog              273   \n",
       "3  t3_c9lci0               65   \n",
       "4  t3_c9i6zr               95   \n",
       "\n",
       "                                           Post Text  Subreddit  \n",
       "0  Research Confirms It: Weak Men Are More Likely...          0  \n",
       "1  The 3 Big Differences Between Conservatives an...          0  \n",
       "2  It's time for trump to do more than lip Servic...          0  \n",
       "3  Koch Groups Renew Push to Reform Broken U.S....          0  \n",
       "4  30 people have been shot in Chicago since the ...          0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:25.378192Z",
     "start_time": "2019-07-07T21:56:25.361856Z"
    }
   },
   "outputs": [],
   "source": [
    "#define features\n",
    "X = master_df['Post Text']\n",
    "y = master_df['Subreddit']\n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:25.674790Z",
     "start_time": "2019-07-07T21:56:25.667775Z"
    }
   },
   "outputs": [],
   "source": [
    "#instantiate Adaboost with learning rate of 0.5 as optimized by GridSearch\n",
    "ada = AdaBoostClassifier(learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:26.109203Z",
     "start_time": "2019-07-07T21:56:25.836365Z"
    }
   },
   "outputs": [],
   "source": [
    "#instantiate TF-IDF and choose optimized hyperparameters from prior section's GridSearch\n",
    "tf= TfidfVectorizer(max_df= 0.4, \n",
    "                max_features= None,\n",
    "                min_df= 3,\n",
    "                ngram_range=(1, 3),\n",
    "                stop_words='english')\n",
    "\n",
    "\n",
    "# Fit our TfidfVectorizer on the training data and transform training data.\n",
    "X_train_tf = pd.DataFrame(tf.fit_transform(X_train).todense()\n",
    "                           ,columns = tf.get_feature_names())\n",
    "\n",
    "# Fit our TfidfVectorizer on the test data and transform training data.\n",
    "X_test_tf = pd.DataFrame(tf.transform(X_test).todense()\n",
    "                           ,columns = tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:27.755911Z",
     "start_time": "2019-07-07T21:56:26.112242Z"
    }
   },
   "outputs": [],
   "source": [
    "#fit the model to our data\n",
    "ada = ada.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:27.768343Z",
     "start_time": "2019-07-07T21:56:27.758863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 1691)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:27.786484Z",
     "start_time": "2019-07-07T21:56:27.772728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:27.895401Z",
     "start_time": "2019-07-07T21:56:27.789640Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7746478873239436"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.score(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:27.952395Z",
     "start_time": "2019-07-07T21:56:27.898774Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7577777777777778"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.score(X_test_tf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression and CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:27.961338Z",
     "start_time": "2019-07-07T21:56:27.955539Z"
    }
   },
   "outputs": [],
   "source": [
    "#instantiate countvectorizer \n",
    "cvec = CountVectorizer(stop_words= new_stop_list,\n",
    "                       ngram_range=(1,2), min_df=2,\n",
    "                       max_features=None, max_df = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:28.124932Z",
     "start_time": "2019-07-07T21:56:27.967326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit our CountVectorizer on the training data and transform training data.\n",
    "X_train_cvec = pd.DataFrame(cvec.fit_transform(X_train).todense()\n",
    "                           ,columns = cvec.get_feature_names())\n",
    "\n",
    "# Fit our CountVectorizer on the test data and transform training data.\n",
    "X_test_cvec = pd.DataFrame(cvec.transform(X_test).todense()\n",
    "                           ,columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:28.209092Z",
     "start_time": "2019-07-07T21:56:28.129300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#instantiate logisticregression\n",
    "lr = LogisticRegression()\n",
    "#fit data\n",
    "lr = lr.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:28.308647Z",
     "start_time": "2019-07-07T21:56:28.299966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 3078)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine and verify shape\n",
    "X_test_cvec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#examine and verify shape\n",
    "X_test_cvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:28.852315Z",
     "start_time": "2019-07-07T21:56:28.843882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine shape to verify a fit\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:29.061831Z",
     "start_time": "2019-07-07T21:56:29.036717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9836916234247591"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score our logistic regression model on our fitted training data\n",
    "lr.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:29.256651Z",
     "start_time": "2019-07-07T21:56:29.231698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7955555555555556"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score our logistic regression model on our fitted testing data\n",
    "lr.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Conceptual Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our models performed well, there are inherent limitations. For starters, we are asked to choose between a model that has very high variance (Logistic Regression) and one that has slightly worse accuracy but much lower variance (Adaboost). We are also limited by the computational requirements of putting every function into a gridsearch in order to tune the hyperparameters towards optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:46.267799Z",
     "start_time": "2019-07-07T21:56:46.261677Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # prepare configuration for cross validation test harness\n",
    "# seed = 42\n",
    "# # prepare models\n",
    "# models = []\n",
    "# models.append(('LR', LogisticRegression()))\n",
    "# models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# models.append(('KNN', KNeighborsClassifier()))\n",
    "# models.append(('CART', DecisionTreeClassifier()))\n",
    "# models.append(('NB', GaussianNB()))\n",
    "# models.append(('SVM', SVC()))\n",
    "# # evaluate each model in turn\n",
    "# results = []\n",
    "# names = []\n",
    "# scoring = 'accuracy'\n",
    "# for name, model in models:\n",
    "# \tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "# \tcv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "# \tresults.append(cv_results)\n",
    "# \tnames.append(name)\n",
    "# \tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "# \tprint(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:46.509115Z",
     "start_time": "2019-07-07T21:56:46.505793Z"
    }
   },
   "outputs": [],
   "source": [
    "# # boxplot algorithm comparison\n",
    "# fig = plt.figure()\n",
    "# fig.suptitle('Algorithm Comparison')\n",
    "# ax = fig.add_subplot(111)\n",
    "# plt.boxplot(results)\n",
    "# ax.set_xticklabels(names)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:46.984041Z",
     "start_time": "2019-07-07T21:56:46.937184Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate predictions\n",
    "pred = ada.predict(X_test_tf)\n",
    "\n",
    "#generate confusion matrix\n",
    "conf = confusion_matrix( y_test,# True values.\n",
    "                     pred)# Predicted values.\n",
    "tn, fp, fn, tp = conf.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:47.499336Z",
     "start_time": "2019-07-07T21:56:47.493196Z"
    }
   },
   "outputs": [],
   "source": [
    "#convert confusion matrix to dataframe\n",
    "df_ada= pd.DataFrame(conf, index =  ['actual republican', 'actual democrats'], columns = ['predicted republican', 'predicted democrats'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix- Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:48.563094Z",
     "start_time": "2019-07-07T21:56:48.551126Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted republican</th>\n",
       "      <th>predicted democrats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual republican</th>\n",
       "      <td>149</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual democrats</th>\n",
       "      <td>56</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   predicted republican  predicted democrats\n",
       "actual republican                   149                   53\n",
       "actual democrats                     56                  192"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides another visualization into the Accuracy score, in which there is approximately 1 in 5 misclassified data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:49.232658Z",
     "start_time": "2019-07-07T21:56:49.217366Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate predictions\n",
    "pred = lr.predict(X_test_cvec)\n",
    "\n",
    "#generate confusion matrix\n",
    "conf = confusion_matrix( y_test,# True values.\n",
    "                     pred)# Predicted values.\n",
    "tn, fp, fn, tp = conf.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:49.410074Z",
     "start_time": "2019-07-07T21:56:49.402127Z"
    }
   },
   "outputs": [],
   "source": [
    "#convert confusion matrix to dataframe\n",
    "df_lr= pd.DataFrame(conf, index =  ['actual republican', 'actual democrats'], columns = ['predicted republican', 'predicted democrats'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:56:49.590196Z",
     "start_time": "2019-07-07T21:56:49.577508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted republican</th>\n",
       "      <th>predicted democrats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual republican</th>\n",
       "      <td>164</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual democrats</th>\n",
       "      <td>54</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   predicted republican  predicted democrats\n",
       "actual republican                   164                   38\n",
       "actual democrats                     54                  194"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our models performed better than the baseline accuracy metric of ~55%, and although almost all of the models displayed different varying degrees of bias, variance and overfitting, the optimal models were LogisticRegression with CountVectorizer. These were determined not only in terms of overall raw accuracy, but in terms of variance and goodness of fit. \n",
    "\n",
    "In recommending this model to be used for the purpose of advertising companies who wish to target potential clients, it is important to weigh the pros and cons of 82.25% accuracy as offered by the Logistic Regression version of our model. This would mean that although 4 out of 5 recipients would be accurate, there would still exist a consistent 1 out of 5 audience that was not actually in the class described by our model. \n",
    "\n",
    "Additional features could also serve to improve the accuracy of our model, three ideas for that in future iterations include:\n",
    "\n",
    "1. Fixing typos or other spelling errors that may have impacted our model's ability to interpret text\n",
    "      \n",
    "2. Incorporating a sentiment analysis aspect, which would involve creating two bags of words in which  we define positive and negative sentiment words, then filter and weight them accordingly.\n",
    "      \n",
    "3. Incorporate a loudness aspect, in which we would look at the prevalence of capital letters in sequence. Although our preprocessing transforms all text to lowercase, there is an argument to be made for the inclusion of series of uppercase text as it usually conveys intense emotion. \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
