{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART TWO \n",
    "\n",
    "## Content List- Part 2\n",
    "\n",
    "- [Data Cleaning and EDA](#Data-Cleaning-and-EDA)\n",
    "- [Preprocessing and Modeling](#Preprocessing-and-Modeling)\n",
    "- [Evaluation and Conceptual Understanding](#Evaluation-and-Conceptual-Understanding)\n",
    "- [Conclusion and Recommendations](#Conclusion-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import text, stop_words\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Dataframe from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv('./data/master_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1778"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "Length of Title    0\n",
       "Post Text          0\n",
       "Subreddit          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for nulls\n",
    "master_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1778, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check shape of new, combined dataframe\n",
    "master_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Length of Title', 'Post Text', 'Subreddit'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.04499437570304"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df['Length of Title'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set feature and targets\n",
    "X = master_df[['Post Text', 'Length of Title']]\n",
    "y = master_df['Subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify= y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Baseline Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Accuracy is our metric, it is vital to determine a baseline so that we can compare our results. We will do this by performing a quick analysis on the distribution of the classes, in order to see if there is any inherent imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.557303\n",
       "0    0.442697\n",
       "Name: Subreddit, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline Accuracy\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline Accuracy of 55.58% is important for the model as it provides a metric on which the model should be judged. 55% is the equivalent of random chance pick by the Majority class, even higher than a coin flip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1333, 2)\n",
      "(1333,)\n",
      "(445, 2)\n",
      "(445,)\n"
     ]
    }
   ],
   "source": [
    "#show us the shape of our data\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amending stop word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n",
      "15\n",
      "342\n",
      "332\n"
     ]
    }
   ],
   "source": [
    "additional_politics_english_stop = ['www', 'things', 'does', 'x200b', 'amp', 'want', 'watch',\n",
    "                           'just', 'like', 'https', 'com', 'trump', 'republican', 'republicans',\n",
    "                           'libertarians', 'democrats', 'democrat', 'people', 'libertarian',\n",
    "                           'says', 'say', 'did', 'this', 'conservative', 'conservatives' ]\n",
    "\n",
    "additional_english_stop = ['www', 'things', 'does', 'x200b', 'amp',\n",
    "                           'just', 'like', 'https', 'com', 'watch', 'want',\n",
    "                           'says', 'say', 'did', 'this']\n",
    "\n",
    "new_stop_list = stop_words.ENGLISH_STOP_WORDS.union(additional_english_stop)\n",
    "new_politics_english_stop_list = stop_words.ENGLISH_STOP_WORDS.union(additional_politics_english_stop)\n",
    "print(len(stop_words.ENGLISH_STOP_WORDS))\n",
    "print(len(additional_english_stop))\n",
    "print(len(new_politics_english_stop_list))\n",
    "print(len(new_stop_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'amp',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'com',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'did',\n",
       "           'do',\n",
       "           'does',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'https',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'just',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'like',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'say',\n",
       "           'says',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'things',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'want',\n",
       "           'was',\n",
       "           'watch',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'www',\n",
       "           'x200b',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_stop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline & GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing gridsearch with vectorizer, add onto X_train the feature desired (length of post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe_cvec_lr = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params_cvec_lr = {\n",
    "    'cvec__max_features': [None,500,1000],\n",
    "    'cvec__min_df': [2,3],\n",
    "    'cvec__max_df': [.3,.4,],\n",
    "    'cvec__ngram_range': [(1,2),(1,3)],\n",
    "    'cvec__stop_words': [None,'english',new_stop_list],\n",
    "    'lr__penalty': ['l2']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_cvec_lr, param_grid=pipe_params_cvec_lr, cv=5,n_jobs = -1,verbose = 1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n",
    "\n",
    "print(f'Best CV Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty strong results with CountVectorizer and Logistic Regression, with a Best CV Score: 0.79581; where the 'cvec__max_df': 0.3, 'cvec__max_features': None, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 3), 'cvec__stop_words':'english', 'lr__penalty': 'l2'. \n",
    "\n",
    "Train Accuracy Score: 0.9798055347793567\n",
    "\n",
    "Test Accuracy Score: 0.8094170403587444\n",
    "\n",
    "The train score of approx 0.9798 was much better than the test score of 0.8094 indicating that this model is overfit despite tuning the hyperparameters and the strong training data score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe_tvec_lr = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params_tvec_lr = {\n",
    "    'tvec__max_features': [None,1000],\n",
    "    'tvec__min_df': [2,3,4],\n",
    "    'tvec__max_df': [.3,.5],\n",
    "    'tvec__ngram_range': [(1,1),(1,3)],\n",
    "    'tvec__stop_words': [None, new_stop_list,'english'],\n",
    "    'lr__penalty': ['l2']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_tvec_lr, param_grid=pipe_params_tvec_lr, cv=4, n_jobs=-1, verbose = 1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for TFIDF and Logistic Regression, with a Best cv score of ~0.7644; where the optimal parameters were 'tvec__max_df': 0.3, 'tvec__max_features': None, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 3), 'tvec__stop_words': new_stop_list, 'lr__penalty': 'l2'.\n",
    "\n",
    "Train Accuracy Score: 0.9281974569932685\n",
    "\n",
    "Test Accuracy Score: 0.7982062780269058\n",
    "\n",
    "The train score was better than the test score indicating that this model is overfit despite tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cvec_mnb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_params_cvec_mnb = {\n",
    "    'cvec__max_features': [None,500,1000,2500],\n",
    "    'cvec__min_df': [2,3],\n",
    "    'cvec__max_df': [.4, .8],\n",
    "    'cvec__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "    'cvec__stop_words': [None, new_stop_list,'english']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_cvec_mnb, param_grid=pipe_params_cvec_mnb, cv=4, n_jobs = 4, verbose = 1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer and Multinomial Naive Bayes, with a Best cv score of 0.7255; where the optimal parameters were 'cvec__max_df': 0.4, 'cvec__max_features': None, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 3), 'cvec__stop_words': 'english'.\n",
    "\n",
    "Train Accuracy Score: 0.8833208676140614\n",
    "\n",
    "Test Accuracy Score: 0.7556053811659192\n",
    "\n",
    "The train score of approx 0.8833 was much better than the test score of 0.7556 indicating that this model is very overfit despite tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tvec_mnb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_params_tvec_mnb = {\n",
    "    'tvec__max_features': [None,500,1000,3000],\n",
    "    'tvec__min_df': [2,3],\n",
    "    'tvec__max_df': [.2,.3,.4,],\n",
    "    'tvec__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "    'tvec__stop_words': [None, new_stop_list,'english']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_tvec_mnb, param_grid=pipe_params_tvec_mnb, cv=4, n_jobs = -1, verbose = 1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Results for TFIDF and Multinomial Naive Bayes, with a Best cv score of 0.7128; where the optimal parameters were 'tvec__max_df': 0.4, 'tvec__max_features': 1000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'.\n",
    "\n",
    "Train Accuracy Score: 0.9012715033657442\n",
    "\n",
    "Test Accuracy Score: 0.7623318385650224\n",
    "\n",
    "The train score of approx 0.9013 was much better than the test score of 0.7623 indicating that this model is very overfit despite tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_pipe = Pipeline([\n",
    "        ('cvec', CountVectorizer()),\n",
    "        ('rfc', RandomForestClassifier())])\n",
    "\n",
    "rf_params = [{\n",
    "    'cvec__max_features': [None, 500,1000],\n",
    "    'cvec__min_df': [2,3],\n",
    "    'cvec__max_df': [.3,.4,.8],\n",
    "    'cvec__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "    'rfc__bootstrap': [True],\n",
    "    'rfc__max_features': [.5, .6],\n",
    "    'rfc__min_samples_leaf': [3,6],\n",
    "    'rfc__min_samples_split':[3,6],\n",
    "    'rfc__n_estimators':[10,100]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(rf_pipe, \n",
    "                   param_grid=rf_params, \n",
    "                   cv = 4,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model drastically improved on variance with the combination of CountVectorizer and RandomForestClassifier. The ideal param: were as follows: 'cvec__max_df': 0.9, 'cvec__max_features': None, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'rfc__bootstrap': True, 'rfc__max_features': 0.5, 'rfc__min_samples_leaf': 4, 'rfc__min_samples_split': 3, 'rfc__n_estimators': 100}\n",
    "\n",
    "Train Accuracy Score: 0.868362004487659\n",
    "\n",
    "Test Accuracy Score: 0.757847533632287\n",
    "\n",
    "Furthermore, the fact that the train accuracy score is still higher than the test accuracy score indicates the model is still overfit, albeit suffering from a lower bias as well as a lower variance than the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([\n",
    "        ('tvec', TfidfVectorizer()),\n",
    "        ('rfc', RandomForestClassifier())])\n",
    "\n",
    "rf_params = [{\n",
    "    'tvec__max_features': [None],\n",
    "    'tvec__min_df': [2,4],\n",
    "    'tvec__max_df': [.3,.4, .5],\n",
    "    'tvec__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "    'tvec__stop_words': [None],\n",
    "    'rfc__bootstrap': [False, True],\n",
    "    'rfc__n_estimators': [10,100],\n",
    "    'rfc__max_features': [.5, .6, .7],\n",
    "    'rfc__min_samples_leaf': [10],\n",
    "    'rfc__min_samples_split':[3]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gs= GridSearchCV(rf_pipe, \n",
    "                   param_grid=rf_params, \n",
    "                   cv = 4,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = 3)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This models score with the combination of TFIDF and RandomForestClassifier average of .7457 was a little lower than the prior model.\n",
    "\n",
    "The ideal paramaters were as follows:{'rfc__bootstrap': False, 'rfc__max_features': 0.5, 'rfc__min_samples_leaf': 10, 'rfc__min_samples_split': 3, 'rfc__n_estimators': 100, 'tvec__max_df': 0.3, 'tvec__max_features': None, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': None.\n",
    "\n",
    "Train Accuracy Score: 0.8160059835452506\n",
    "\n",
    "Test Accuracy Score: 0.7309417040358744\n",
    "\n",
    "Furthermore, the fact that the train accuracy score is still higher than the test accuracy score indicates the model is still overfit (0.78608 vs 0.7511) , albeit suffering from a lower bias as well as a lower variance than the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ada_pipe = Pipeline([\n",
    "        ('cvec', CountVectorizer()),\n",
    "        ('ada', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "ada_params = {\n",
    "    'cvec__max_features': [None,500,1000],\n",
    "    'cvec__min_df': [3,5],\n",
    "    'cvec__max_df': [.4,.3],\n",
    "    'cvec__ngram_range': [(1,2),(2,3),(1,3)],\n",
    "    'cvec__stop_words': [None, 'english', new_stop_list],\n",
    "    'ada__learning_rate': [0.3,.5,.7]}\n",
    "\n",
    "gs= GridSearchCV(ada_pipe, \n",
    "                   param_grid=ada_params, \n",
    "                   cv = 5,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ada_pipe = Pipeline([\n",
    "        ('tvec', TfidfVectorizer()),\n",
    "        ('ada', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "ada_params = {\n",
    "    'tvec__max_features': [None,500,1000],\n",
    "    'tvec__min_df': [2,3,4],\n",
    "    'tvec__max_df': [.5,.4,.3],\n",
    "    'tvec__ngram_range': [(1,1),(1,3)],\n",
    "    'tvec__stop_words': [None, 'english', new_stop_list],\n",
    "    'ada__learning_rate': [.5]}\n",
    "\n",
    "gs= GridSearchCV(ada_pipe, \n",
    "                   param_grid=ada_params, \n",
    "                   cv = 3,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost with TFIDF proved the best so far, with lower variance and higher accuracy with optimal settings of:'ada__learning_rate': 0.5, 'tvec__max_df': 0.5, 'tvec__max_features': 500, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 3), 'tvec__stop_words': new_stop_list.\n",
    "\n",
    "Train Accuracy Score: 0.8032909498878086\n",
    "\n",
    "Test Accuracy Score: 0.7645739910313901\n",
    "\n",
    "Scores show that there is still a tiny bit of overfit, but all in all this model should generalize the best to new data and so we will make our predictions using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   45.5s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7569392348087022\n",
      "Best Parameters: {'cvec__max_df': 0.4, 'cvec__max_features': None, 'cvec__min_df': 5, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'from', 'might', 'hundred', 'each', 'sixty', 'five', 'elsewhere', 'ours', 'was', 'too', 'herself', 'found', 'ie', 'nor', 'fifteen', 'onto', 'i', 'herein', 'name', 'yet', 'sometimes', 'sometime', 'always', 'there', 'became', 'first', 'just', 'beside', 'un', 'via', 'once', 'last', 'of', 'couldnt', 'more', 'am', 'mine', 'done', 'inc', 'every', 'your', 'upon', 'again', 'amount', 'another', 'this', 'all', 'we', 'off', 'hasnt', 'had', 'become', 'fire', 'whole', 'becoming', 'while', 'things', 'myself', 'since', 'these', 'indeed', 'is', 'did', 'because', 'show', 'seemed', 'formerly', 'amoungst', 'in', 'itself', 'below', 'due', 'hereby', 'sincere', 'further', 'at', 'whereas', 'anyone', 'otherwise', 'least', 'some', 'someone', 'himself', 'those', 'eight', 'her', 'although', 'forty', 'which', 'thus', 'whereafter', 'www', 'it', 'move', 'detail', 'our', 'thru', 'were', 'whereupon', 'but', 'therefore', 'alone', 'con', 'made', 'if', 'go', 'everywhere', 'afterwards', 'neither', 'like', 'as', 'whither', 'front', 'up', 'anything', 'who', 'serious', 'cant', 'would', 'etc', 'three', 'third', 'thereafter', 'can', 'amp', 'hereafter', 'two', 'one', 'its', 'wherever', 'take', 'much', 'say', 'among', 'no', 'together', 'want', 'amongst', 'whence', 'anyway', 'fill', 'com', 'keep', 'find', 'whenever', 'less', 'themselves', 'latterly', 'them', 'whether', 'does', 'fifty', 'full', 'says', 'however', 'my', 'yourselves', 'beforehand', 'four', 'rather', 'without', 'so', 'seeming', 'nowhere', 'latter', 'system', 'noone', 'throughout', 'wherein', 'yours', 'many', 'until', 'the', 'during', 'something', 'are', 'co', 'ever', 'beyond', 'perhaps', 'down', 'few', 'six', 'have', 'any', 'same', 'when', 'bottom', 'watch', 'thereupon', 'therein', 'several', 'never', 'still', 'back', 'eg', 'under', 'put', 'be', 'seem', 'for', 'across', 'enough', 'an', 'ltd', 'though', 'twelve', 'almost', 'whom', 'empty', 'somewhere', 'then', 'also', 'us', 'being', 'should', 'describe', 'besides', 'to', 'very', 'on', 'about', 'than', 'him', 'with', 'nobody', 'what', 'both', 'most', 'could', 'often', 'has', 'now', 'meanwhile', 'anywhere', 'they', 'other', 'their', 'hereupon', 'she', 'everyone', 'toward', 'against', 'whoever', 'he', 'must', 'becomes', 'thereby', 'eleven', 'nine', 'may', 'within', 'everything', 'bill', 'thick', 'part', 'ourselves', 'me', 'into', 'already', 'give', 'whose', 'even', 'see', 'somehow', 'interest', 'yourself', 'none', 'over', 'behind', 'whatever', 'how', 'namely', 'former', 'around', 'per', 'thin', 'either', 'along', 'else', 'or', 'well', 'hence', 'towards', 'you', 'do', 'here', 'anyhow', 'side', 'x200b', 'next', 'his', 'own', 'thence', 'a', 'not', 'top', 'de', 'mill', 'cry', 'nevertheless', 'where', 'above', 'ten', 'only', 'through', 'twenty', 'been', 'out', 'before', 'by', 'except', 'nothing', 'whereby', 'please', 're', 'such', 'cannot', 'that', 'why', 'will', 'mostly', 'get', 'between', 'others', 'after', 'https', 'seems', 'and', 'moreover', 'hers', 'call'})}\n",
      "Train Accuracy Score: 0.7944486121530383\n",
      "Test Accuracy Score: 0.7415730337078652\n"
     ]
    }
   ],
   "source": [
    "xgb_pipe = Pipeline([\n",
    "        ('cvec', CountVectorizer()),\n",
    "        ('xgb', XGBClassifier())\n",
    "])\n",
    "\n",
    "xgb_params = {\n",
    "    'cvec__max_features': [None,500,1000],\n",
    "    'cvec__min_df': [3,5],\n",
    "    'cvec__max_df': [.4,.3],\n",
    "    'cvec__ngram_range': [(1,2),(2,3),(1,3)],\n",
    "    'cvec__stop_words': [None, 'english', new_stop_list]}\n",
    "\n",
    "gs= GridSearchCV(xgb_pipe, \n",
    "                   param_grid= xgb_params, \n",
    "                   cv = 5,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 486 out of 486 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.741185296324081\n",
      "Best Parameters: {'tvec__max_df': 0.5, 'tvec__max_features': 1000, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 3), 'tvec__stop_words': 'english'}\n",
      "Train Accuracy Score: 0.8132033008252063\n",
      "Test Accuracy Score: 0.7415730337078652\n"
     ]
    }
   ],
   "source": [
    "xgb_pipe = Pipeline([\n",
    "        ('tvec', TfidfVectorizer()),\n",
    "        ('xgb', XGBClassifier())\n",
    "])\n",
    "\n",
    "xgb_params = {\n",
    "    'tvec__max_features': [None,500,1000],\n",
    "    'tvec__min_df': [2,3,4],\n",
    "    'tvec__max_df': [.5,.4,.3],\n",
    "    'tvec__ngram_range': [(1,1),(1,3)],\n",
    "    'tvec__stop_words': [None, 'english', new_stop_list]}\n",
    "\n",
    "gs= GridSearchCV(xgb_pipe, \n",
    "                   param_grid=xgb_params, \n",
    "                   cv = 3,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "gs.fit(X_train['Post Text'],y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')\n",
    "print(f'Train Accuracy Score: {gs.score(X_train[\"Post Text\"],y_train)}')\n",
    "print(f'Test Accuracy Score: {gs.score(X_test[\"Post Text\"],y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions Utilizing the Top Performing Models\n",
    "#### I would classify two of the models as the best, the one with the highest overall score (lowest bias) and the one with the smallest overall difference between the train and test data (lowest variance). These models are tested out below with their optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Length of Title</th>\n",
       "      <th>Post Text</th>\n",
       "      <th>Subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_bafvy6</td>\n",
       "      <td>293</td>\n",
       "      <td>In her own words... Antonia Okafor responds to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_baf5b7</td>\n",
       "      <td>54</td>\n",
       "      <td>Science shows that white liberals condescend t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_bah4s9</td>\n",
       "      <td>20</td>\n",
       "      <td>That's not racist...\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_bagod9</td>\n",
       "      <td>237</td>\n",
       "      <td>Sir Mick Jagger had a heart valve problem and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_bah4s7</td>\n",
       "      <td>24</td>\n",
       "      <td>ðŸŽ¶My Heart Will Go OnnnnðŸŽ¶\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  Length of Title  \\\n",
       "0  t3_bafvy6              293   \n",
       "1  t3_baf5b7               54   \n",
       "2  t3_bah4s9               20   \n",
       "3  t3_bagod9              237   \n",
       "4  t3_bah4s7               24   \n",
       "\n",
       "                                           Post Text  Subreddit  \n",
       "0  In her own words... Antonia Okafor responds to...          0  \n",
       "1  Science shows that white liberals condescend t...          0  \n",
       "2                             That's not racist...\\n          0  \n",
       "3  Sir Mick Jagger had a heart valve problem and ...          0  \n",
       "4                         ðŸŽ¶My Heart Will Go OnnnnðŸŽ¶\\n          0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define features\n",
    "X = master_df['Post Text']\n",
    "y = master_df['Subreddit']\n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate Adaboost with learning rate of 0.5 as optimized by GridSearch\n",
    "ada = AdaBoostClassifier(learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate TF-IDF and choose optimized hyperparameters from prior section's GridSearch\n",
    "tf= TfidfVectorizer(max_df= 0.4, \n",
    "                max_features= None,\n",
    "                min_df= 3,\n",
    "                ngram_range=(1, 3),\n",
    "                stop_words='english')\n",
    "\n",
    "\n",
    "# Fit our TfidfVectorizer on the training data and transform training data.\n",
    "X_train_tf = pd.DataFrame(tf.fit_transform(X_train).todense()\n",
    "                           ,columns = tf.get_feature_names())\n",
    "\n",
    "# Fit our TfidfVectorizer on the test data and transform training data.\n",
    "X_test_tf = pd.DataFrame(tf.transform(X_test).todense()\n",
    "                           ,columns = tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model to our data\n",
    "ada = ada.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445, 1793)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8079519879969993"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.score(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7617977528089888"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.score(X_test_tf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression and CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate countvectorizer \n",
    "cvec = CountVectorizer(stop_words= new_stop_list,\n",
    "                       ngram_range=(1,2), min_df=2,\n",
    "                       max_features=None, max_df = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our CountVectorizer on the training data and transform training data.\n",
    "X_train_cvec = pd.DataFrame(cvec.fit_transform(X_train).todense()\n",
    "                           ,columns = cvec.get_feature_names())\n",
    "\n",
    "# Fit our CountVectorizer on the test data and transform training data.\n",
    "X_test_cvec = pd.DataFrame(cvec.transform(X_test).todense()\n",
    "                           ,columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#instantiate logisticregression\n",
    "lr = LogisticRegression()\n",
    "#fit data\n",
    "lr = lr.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445, 3212)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine and verify shape\n",
    "X_test_cvec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#examine and verify shape\n",
    "X_test_cvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine shape to verify a fit\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789947486871718"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score our logistic regression model on our fitted training data\n",
    "lr.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8224719101123595"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score our logistic regression model on our fitted testing data\n",
    "lr.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Conceptual Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our models performed well, there are inherent limitations. For starters, we are asked to choose between a model that has very high variance (Logistic Regression) and one that has slightly worse accuracy but much lower variance (Adaboost). We are also limited by the computational requirements of putting every function into a gridsearch in order to tune the hyperparameters towards optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate predictions\n",
    "pred = ada.predict(X_test_tf)\n",
    "\n",
    "#generate confusion matrix\n",
    "conf = confusion_matrix( y_test,# True values.\n",
    "                     pred)# Predicted values.\n",
    "tn, fp, fn, tp = conf.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert confusion matrix to dataframe\n",
    "df_ada= pd.DataFrame(conf, index =  ['actual republican', 'actual democrats'], columns = ['predicted republican', 'predicted democrats'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix- Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted republican</th>\n",
       "      <th>predicted democrats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual republican</th>\n",
       "      <td>150</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual democrats</th>\n",
       "      <td>59</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   predicted republican  predicted democrats\n",
       "actual republican                   150                   47\n",
       "actual democrats                     59                  189"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides another visualization into the Accuracy score, in which there is approximately 1 in 5 misclassified data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate predictions\n",
    "pred = lr.predict(X_test_cvec)\n",
    "\n",
    "#generate confusion matrix\n",
    "conf = confusion_matrix( y_test,# True values.\n",
    "                     pred)# Predicted values.\n",
    "tn, fp, fn, tp = conf.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert confusion matrix to dataframe\n",
    "df_lr= pd.DataFrame(conf, index =  ['actual republican', 'actual democrats'], columns = ['predicted republican', 'predicted democrats'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted republican</th>\n",
       "      <th>predicted democrats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual republican</th>\n",
       "      <td>163</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual democrats</th>\n",
       "      <td>45</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   predicted republican  predicted democrats\n",
       "actual republican                   163                   34\n",
       "actual democrats                     45                  203"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our models performed better than the baseline accuracy metric of ~55%, and although almost all of the models displayed different varying degrees of bias, variance and overfitting, the optimal models were LogisticRegression with CountVectorizer. These were determined not only in terms of overall raw accuracy, but in terms of variance and goodness of fit. \n",
    "\n",
    "In recommending this model to be used for the purpose of advertising companies who wish to target potential clients, it is important to weigh the pros and cons of 82.25% accuracy as offered by the Logistic Regression version of our model. This would mean that although 4 out of 5 recipients would be accurate, there would still exist a consistent 1 out of 5 audience that was not actually in the class described by our model. \n",
    "\n",
    "Additional features could also serve to improve the accuracy of our model, three ideas for that in future iterations include:\n",
    "\n",
    "1. Fixing typos or other spelling errors that may have impacted our model's ability to interpret text\n",
    "      \n",
    "2. Incorporating a sentiment analysis aspect, which would involve creating two bags of words in which  we define positive and negative sentiment words, then filter and weight them accordingly.\n",
    "      \n",
    "3. Incorporate a loudness aspect, in which we would look at the prevalence of capital letters in sequence. Although our preprocessing transforms all text to lowercase, there is an argument to be made for the inclusion of series of uppercase text as it usually conveys intense emotion. \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
